'''
    Midterm: Clustering - Digits Dataset
    Joseph Nelson Farrell  
    DS 5230 Unsupervised Machine Learning  
    Professor Stevin Morin, PhD  
    03-07-24

    This file contains a function that will execute Kmeans and DBscan looking for the best clustering
    algorithm. Kmeans will be executed first, if Kmeans does not successfully find the clusters
    (based on various metrics), DBscan will be searched. 

    Either best silhouette score or best validity index will be used as a metric.
'''
import numpy as np
import pandas as pd
from sklearn import metrics
from sklearn.cluster import KMeans
from sklearn.cluster import DBSCAN
import hdbscan.validity as dbcv_hdbscan
from sklearn.cluster import DBSCAN

from clustering_utils import generate_hopkins_statistic
from clustering_utils import find_elbow
from clustering_utils import get_best_eps_and_k

def clustering(results_dict: dict) -> dict:
    """ 
        Functions: clustering

        Function to find the best clusterung algorithm. Kmeans will be tested and checked first, if kmeans fails to find a viable cluster, 
            DBscan will executed.

        Parameters:
        ___________

        results_dict : (dict) This is the results dict generated by umap_dim_red

        Returns:
        ________

        row_dict: (dict) A dict containing the parameters of the embedding and the metrics of the best cluster.
            
    """
    # create a results_df from results_dict
    results_dict_df = pd.DataFrame(results_dict)

    # extract the embedding
    embedding = results_dict_df.embedding.to_numpy()
    embedding = embedding[0]

    # compute the hopkins statistic
    hopkins_stat = generate_hopkins_statistic(embedding)
 
    # declare max clusters
    max_clusters = 15

    # instantiate results dict to store results from kmeans
    k_means_results_dict = {
        "n_clusters": [],
        "silhouette_score": [],
        "db_score": [],
        "ch_score": [],
        "inertia": [],
        "cluster_labels": []}
    
    # iterate over max_clusters
    for i in range(2, max_clusters + 1):

        # instantiate kmeans
        k_means = KMeans(
                        n_clusters = i, 
                        init = "k-means++", 
                        n_init = "auto", 
                        max_iter = 300, 
                        verbose = 0, 
                        random_state = 42, 
                        copy_x = True, 
                        algorithm = "lloyd")
        
        # fit kmeans
        k_means.fit_predict(embedding)

        # compute metrics and gather
        silhouette_score = metrics.silhouette_score(embedding, k_means.labels_)
        ch_score =  metrics.calinski_harabasz_score(embedding, k_means.labels_)
        db_score = metrics.davies_bouldin_score(embedding, k_means.labels_)
        interia = k_means.inertia_
        cluster_labels = k_means.labels_

        # update kmeans results dict
        k_means_results_dict["n_clusters"].append(i)
        k_means_results_dict["silhouette_score"].append(silhouette_score)
        k_means_results_dict["db_score"].append(db_score)
        k_means_results_dict["ch_score"].append(ch_score)
        k_means_results_dict["inertia"].append(interia)
        k_means_results_dict["cluster_labels"].append(cluster_labels)

    # create kmeans results dataframe and k_vs_interia_frame
    k_means_results_frame = pd.DataFrame(k_means_results_dict)
    k_vs_inertia_df = k_means_results_frame[['n_clusters', "inertia"]]

    # pull out the index of relevant scores
    db_score_min_index = k_means_results_frame['db_score'].idxmin()
    ch_score_max_index = k_means_results_frame['ch_score'].idxmax()
    sil_score_max_index = k_means_results_frame['silhouette_score'].idxmax()

    # get the max silhouette score
    silhouette_score = k_means_results_frame['silhouette_score'][sil_score_max_index]

    # get the number of clusters found at the index of scores
    n_clusters_db_score_is_min = k_means_results_frame["n_clusters"][db_score_min_index]
    n_clusters_ch_score_is_max = k_means_results_frame["n_clusters"][ch_score_max_index]
    n_clusters_silhouette_score_is_max = k_means_results_frame["n_clusters"][sil_score_max_index]
    cluster_labels = k_means_results_frame["cluster_labels"][sil_score_max_index]
    n_clusters_found = find_elbow(k_vs_inertia_df)

    # get the labels
    cluster_labels = k_means_results_frame["cluster_labels"][sil_score_max_index]

    # conditions check variables
    all_4_match = (n_clusters_db_score_is_min == n_clusters_ch_score_is_max == n_clusters_silhouette_score_is_max == n_clusters_found)
    just_3_match =  (n_clusters_db_score_is_min == n_clusters_ch_score_is_max == n_clusters_silhouette_score_is_max)

    # perform check of kmeans results
    if (all_4_match or just_3_match):
        if all_4_match:
            print(f'All 4 Conditons Match')
        else:
            print(f'3 of 4 Conditions Match')

            # instantiate row_dict
        row_dict = {
                    'true_number_of_clusters': 10,
                    'algo': "kmeans",
                    'n_clusters_found': n_clusters_found,
                    "n_clusters_db_score_is_min": n_clusters_db_score_is_min,
                    "n_clusters_ch_score_is_max": n_clusters_ch_score_is_max,
                    "n_clusters_silhouette_score_is_max": n_clusters_silhouette_score_is_max,
                    "silhouette_score": silhouette_score,
                    "hopkins_statistic": hopkins_stat,
                    "umap_n_neighbors": results_dict_df['n_neighbors'][0],
                    "umap_min_dist": results_dict_df['min_dist'][0],
                    "umap_metric": results_dict_df['metric'][0],
                    "umap_n_components": results_dict_df['n_components'][0],
                    "trustworthiness": results_dict_df['trustworthiness'][0],
                    "eps": np.nan,
                    "dbscan_min_samples": np.nan,
                    "validity_index": np.nan,
                    "cluster_labels": cluster_labels
                    }

        aux_dict = {
            "embedding": embedding,
            "cluster_labels": cluster_labels,
            "n_components": results_dict_df['n_components'][0]
            }

    # if kmeans does not find cluster, move on to dbscan
    else:

        # instantiate dbscan dict
        db_scan_dict = {
                        "eps": [],
                        "db_scan_min_samples": [],
                        "n_clusters_found": [],
                        "validity_index": [],
                        "cluster_labels": []
                        }
        
        # find best eps and min samples
        eps, min_samples = get_best_eps_and_k(embedding)
        eps_scan_range = np.arange(1.0, 1.8, 0.1)
        
        # iterate over eps scan range
        for i in eps_scan_range:

            # set eps
            f_eps = i * eps

            # instantiate dbscan
            dbscan = DBSCAN(
                            eps = f_eps,
                            min_samples = min_samples,
                            metric = 'euclidean',
                            metric_params = None,
                            algorithm = 'auto',
                            leaf_size = 30,
                            p = None,
                            n_jobs = None
                            )
            
            # fit dbscan
            dbscan.fit(embedding)

            # extract cluster labels
            cluster_labels = dbscan.labels_

            # compute number of clusters
            n_clusters = len(np.unique(dbscan.labels_)) - (1 if -1 in dbscan.labels_ else 0)

            # change dtype of embedding for validity_index function
            embedding = embedding.astype(np.float64)

            # check if there is a viable validity index
            try:
                validity_index = dbcv_hdbscan.validity_index(embedding, dbscan.labels_)
            except ValueError as e:
                validity_index = np.nan

            # update ddb_scan_results_dict
            db_scan_dict["eps"].append(f_eps)
            db_scan_dict["db_scan_min_samples"].append(min_samples)
            db_scan_dict["n_clusters_found"].append(n_clusters)
            db_scan_dict["validity_index"].append(validity_index)
            db_scan_dict["cluster_labels"].append(cluster_labels)
        
        # convert dbscan dict to dataframe
        db_scan_results_frame = pd.DataFrame(db_scan_dict)

        # get the index of max validity score
        validity_max_index = db_scan_results_frame["validity_index"].idxmax()

        row_dict = {
            'true_number_of_clusters': 10,
            'algo': "DBScan",
            'n_clusters_found': db_scan_results_frame["n_clusters_found"][validity_max_index],
            "n_clusters_db_score_is_min": np.nan,
            "n_clusters_ch_score_is_max": np.nan,
            "n_clusters_silhouette_score_is_max": np.nan,
            "silhouette_score": np.nan,
            "hopkins_statistic": hopkins_stat,
            "umap_n_neighbors": results_dict_df['n_neighbors'][0],
            "umap_min_dist": results_dict_df['min_dist'][0],
            "umap_metric": results_dict_df['metric'][0],
            "umap_n_components": results_dict_df['n_components'][0],
            "trustworthiness": results_dict_df['trustworthiness'][0],
            "eps": db_scan_results_frame["eps"][validity_max_index],
            "dbscan_min_samples": db_scan_results_frame["db_scan_min_samples"][validity_max_index],
            "validity_index": db_scan_results_frame["validity_index"][validity_max_index],
            "cluster_labels":  db_scan_results_frame["cluster_labels"][validity_max_index]
            }
        
        # apppend aux dict
        #aux_dict["embedding"].append(embedding)
        #aux_dict["cluster_labels"].append(cluster_labels)
        #aux_dict["n_compenents"].append(results_dict_df['n_components'][0])

        aux_dict = {
            "embedding": embedding,
            "cluster_labels": cluster_labels,
            "n_components": results_dict_df['n_components'][0]
            }
        
    return row_dict, aux_dict